{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjqGtEqMUB2O"
      },
      "source": [
        "<img src=\"https://www.uc3m.es/ss/Satellite?blobcol=urldata&blobkey=id&blobtable=MungoBlobs&blobwhere=1371573952659\">\n",
        "\n",
        "---\n",
        "\n",
        "# WEB ANALYTICS COURSE 4 - SEMESTER 2\n",
        "# BACHELOR IN DATA SCIENCE AND ENGINEERING\n",
        "\n",
        "# LAB 1.2 WEB SCRAPING WITH SCRAPY\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vS1O4BW82LV6"
      },
      "source": [
        "# 0. Lab Preparation\n",
        "\n",
        "1.  Study and have clear the concepts explained in the theoretical class and the introductory lab.\n",
        "\n",
        "2.   Gain experience with the use of the [Scrapy](https://scrapy.org/). The exercises of this lab will be mainly based on the utilization of functions offered by this library.\n",
        "\n",
        "3. It is assumed students have experience in using Python notebooks. Either a local installation (e.g., local python installation + Jupyter) or a cloud-based solution (e.g., Google Colab). *We recommend the second option*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwlGXDUG2db2"
      },
      "source": [
        "# 1. Lab Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypNv8Fpi2YS7"
      },
      "source": [
        "* In this lab, we will implement a web scraper using [Scrapy](https://scrapy.org/). One of the tools explained in the theoretical class.\n",
        "\n",
        "* The lab will be done in groups of 4 people.\n",
        "\n",
        "* The lab defines a set of milestones the students must complete. Upon completing all the milestones, students should call the professor, who will check the correctness of the solution (*If the professor is busy, do not wait for them, move to the next lab*).\n",
        "\n",
        "* **The final mark will be computed as a function of the number of milestones successfully completed.**\n",
        "\n",
        "* **Each group should also share their lab notebook with the professor upon the finalization of the lab.**\n",
        "\n",
        "* In this lab we will use the [Scrapy](https://scrapy.org/) library for the creation of a web scraper, to extract information from the web. As indicated in the *Lab Preparation* section above, it is expected that students have gained experience in the use of the library before starting the first session of the lab.\n",
        "\n",
        "- It is recommended to use [Google Colab](https://colab.research.google.com/) to produce the Python notebook with the solution of the lab. Of course, if any student prefers using its local programming environment (e.g., jupyter) and python installation, they are welcome to do so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "lnIUv5WsKSfJ",
        "outputId": "65101435-30d5-425d-e336-eda5d43c7f25"
      },
      "outputs": [],
      "source": [
        "!pip install scrapy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-VLEo2MPCiRQ"
      },
      "outputs": [],
      "source": [
        "import scrapy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhPF9V3_Jyjv"
      },
      "source": [
        "# MILESTONE 1\n",
        "\n",
        "a) Create a _Scrapy_ project for this lab (**HINT:** Remember you can use the command `startproject`).\n",
        "\n",
        "b) Create a crawler/spider to the website [BACHELOR IN DATA SCIENCE AND ENGINEERING\n",
        "](https://www.uc3m.es/bachelor-degree/data-science).\n",
        "\n",
        "c) Add the code to the crawler to get PROGRAM header. **TIP:** Find the element tag with `id=\"program\"` and print the result.\n",
        "\n",
        "d) Add the code to the crawler to find the table inside PROGRAM for Course 1 - Semester 1 and print the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QR58vq8AwZuQ",
        "outputId": "7e063b92-345b-44af-a446-d7a56cfa7af6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: scrapy.cfg already exists in C:\\Users\\algar\\Documents\\Universitat\\WebAnalytics\\Web Scraping\\Scrapy Lab\\scrapy_lab\n"
          ]
        }
      ],
      "source": [
        "# a) Create a Scrapy project for this lab\n",
        "!scrapy startproject scrapy_lab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcpQL0jgXBPq",
        "outputId": "0c6fecb0-b053-4de5-a0c6-d18b48426faf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\algar\\Documents\\Universitat\\WebAnalytics\\Web Scraping\\Scrapy Lab\\scrapy_lab\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\algar\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
            "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
          ]
        }
      ],
      "source": [
        "# Move to the project folder\n",
        "%cd \"scrapy_lab\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkBV7gekXBMB",
        "outputId": "0d951e9e-8ee9-4e18-9d8c-e8560c948fa7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spider 'datascience_spider' already exists in module:\n",
            "  scrapy_lab.spiders.datascience_spider\n"
          ]
        }
      ],
      "source": [
        "# b) Create a crawler/spider\n",
        "!scrapy genspider datascience_spider https://www.uc3m.es/bachelor-degree/data-science"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qMhG0vJxDzjv"
      },
      "outputs": [],
      "source": [
        "# c) Add the code to the crawler to get PROGRAM header. TIP: Find the element tag with id=\"program\" and print the result.\n",
        "class DatascienceSpider(scrapy.Spider):\n",
        "    name = 'datascience_spider'\n",
        "    allowed_domains = ['uc3m.es']\n",
        "    start_urls = ['https://www.uc3m.es/bachelor-degree/data-science']\n",
        "\n",
        "    def parse(self, response):\n",
        "        # Extract the header with id=\"program\"\n",
        "        program_header = response.xpath('//*[@id=\"program\"]/h2/text()').get()\n",
        "\n",
        "        # Print the result\n",
        "        if program_header:\n",
        "            print(f\"Program Header: {program_header}\")\n",
        "        else:\n",
        "            print(\"Program header not found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kJBpQpbQ2C3b",
        "outputId": "e13e2ec1-9e10-4904-fafb-d6abaa193b48"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-09-25 13:36:10 [scrapy.utils.log] INFO: Scrapy 2.11.2 started (bot: scrapy_lab)\n",
            "2024-09-25 13:36:10 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.11.7, cssselect 1.2.0, parsel 1.9.1, w3lib 2.2.1, Twisted 24.7.0, Python 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)], pyOpenSSL 24.2.1 (OpenSSL 3.3.2 3 Sep 2024), cryptography 43.0.1, Platform Windows-10-10.0.22631-SP0\n",
            "2024-09-25 13:36:10 [scrapy.addons] INFO: Enabled addons:\n",
            "[]\n",
            "2024-09-25 13:36:10 [asyncio] DEBUG: Using selector: SelectSelector\n",
            "2024-09-25 13:36:10 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
            "2024-09-25 13:36:10 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop\n",
            "2024-09-25 13:36:10 [scrapy.extensions.telnet] INFO: Telnet Password: 27343ec731afcbaa\n",
            "2024-09-25 13:36:11 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2024-09-25 13:36:11 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'BOT_NAME': 'scrapy_lab',\n",
            " 'FEED_EXPORT_ENCODING': 'utf-8',\n",
            " 'NEWSPIDER_MODULE': 'scrapy_lab.spiders',\n",
            " 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n",
            " 'ROBOTSTXT_OBEY': True,\n",
            " 'SPIDER_MODULES': ['scrapy_lab.spiders'],\n",
            " 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n",
            "2024-09-25 13:36:11 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2024-09-25 13:36:11 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2024-09-25 13:36:11 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2024-09-25 13:36:11 [scrapy.core.engine] INFO: Spider opened\n",
            "2024-09-25 13:36:11 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2024-09-25 13:36:11 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2024-09-25 13:36:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.uc3m.es/robots.txt> (referer: None)\n",
            "2024-09-25 13:36:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.uc3m.es/bachelor-degree/data-science> (referer: None)\n",
            "2024-09-25 13:36:12 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2024-09-25 13:36:12 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 462,\n",
            " 'downloader/request_count': 2,\n",
            " 'downloader/request_method_count/GET': 2,\n",
            " 'downloader/response_bytes': 18391,\n",
            " 'downloader/response_count': 2,\n",
            " 'downloader/response_status_count/200': 2,\n",
            " 'elapsed_time_seconds': 0.889524,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2024, 9, 25, 11, 36, 12, 368382, tzinfo=datetime.timezone.utc),\n",
            " 'httpcompression/response_bytes': 92807,\n",
            " 'httpcompression/response_count': 2,\n",
            " 'log_count/DEBUG': 5,\n",
            " 'log_count/INFO': 10,\n",
            " 'response_received_count': 2,\n",
            " 'robotstxt/request_count': 1,\n",
            " 'robotstxt/response_count': 1,\n",
            " 'robotstxt/response_status_count/200': 1,\n",
            " 'scheduler/dequeued': 1,\n",
            " 'scheduler/dequeued/memory': 1,\n",
            " 'scheduler/enqueued': 1,\n",
            " 'scheduler/enqueued/memory': 1,\n",
            " 'start_time': datetime.datetime(2024, 9, 25, 11, 36, 11, 478858, tzinfo=datetime.timezone.utc)}\n",
            "2024-09-25 13:36:12 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ]
        }
      ],
      "source": [
        "!scrapy crawl datascience_spider"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "wXcrEkSJDzMG"
      },
      "outputs": [],
      "source": [
        "# d) Add the code to the crawler to find the table inside PROGRAM for Course 1 - Semester 1 and print the result.\n",
        "class DatascienceSpider(scrapy.Spider):\n",
        "    name = 'datascience_spider'\n",
        "    allowed_domains = ['uc3m.es']\n",
        "    start_urls = ['https://www.uc3m.es/bachelor-degree/data-science']\n",
        "\n",
        "    def parse(self, response):\n",
        "        # Step 1: Find the \"Year 1 - Semester 1\" table by locating the h3 with \"Year 1 - Semester 1\"\n",
        "        table = response.xpath('//h3[contains(text(), \"Year 1 - Semester 1\")]/following-sibling::table')\n",
        "\n",
        "        # Step 2: Extract rows from the table\n",
        "        rows = table.xpath('.//tr')\n",
        "\n",
        "        # Step 3: Loop through each row and extract cell data\n",
        "        for row in rows:\n",
        "            # Extract all cells (td or th)\n",
        "            cells = row.xpath('.//td | .//th').xpath('string()').getall()\n",
        "\n",
        "            # Print the row data\n",
        "            print(f\"Row: {cells}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5R_5VKPA2v_R",
        "outputId": "ff9bf9c4-a29a-48ad-a331-ab93d8fb8ad3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-09-25 13:36:17 [scrapy.utils.log] INFO: Scrapy 2.11.2 started (bot: scrapy_lab)\n",
            "2024-09-25 13:36:17 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.11.7, cssselect 1.2.0, parsel 1.9.1, w3lib 2.2.1, Twisted 24.7.0, Python 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)], pyOpenSSL 24.2.1 (OpenSSL 3.3.2 3 Sep 2024), cryptography 43.0.1, Platform Windows-10-10.0.22631-SP0\n",
            "2024-09-25 13:36:17 [scrapy.addons] INFO: Enabled addons:\n",
            "[]\n",
            "2024-09-25 13:36:17 [asyncio] DEBUG: Using selector: SelectSelector\n",
            "2024-09-25 13:36:17 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
            "2024-09-25 13:36:17 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop\n",
            "2024-09-25 13:36:17 [scrapy.extensions.telnet] INFO: Telnet Password: db25e02fbc4ea855\n",
            "2024-09-25 13:36:17 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2024-09-25 13:36:17 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'BOT_NAME': 'scrapy_lab',\n",
            " 'FEED_EXPORT_ENCODING': 'utf-8',\n",
            " 'NEWSPIDER_MODULE': 'scrapy_lab.spiders',\n",
            " 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n",
            " 'ROBOTSTXT_OBEY': True,\n",
            " 'SPIDER_MODULES': ['scrapy_lab.spiders'],\n",
            " 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n",
            "2024-09-25 13:36:17 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2024-09-25 13:36:17 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2024-09-25 13:36:17 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2024-09-25 13:36:17 [scrapy.core.engine] INFO: Spider opened\n",
            "2024-09-25 13:36:17 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2024-09-25 13:36:17 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2024-09-25 13:36:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.uc3m.es/robots.txt> (referer: None)\n",
            "2024-09-25 13:36:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.uc3m.es/bachelor-degree/data-science> (referer: None)\n",
            "2024-09-25 13:36:18 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2024-09-25 13:36:18 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 462,\n",
            " 'downloader/request_count': 2,\n",
            " 'downloader/request_method_count/GET': 2,\n",
            " 'downloader/response_bytes': 18411,\n",
            " 'downloader/response_count': 2,\n",
            " 'downloader/response_status_count/200': 2,\n",
            " 'elapsed_time_seconds': 0.851637,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2024, 9, 25, 11, 36, 18, 761600, tzinfo=datetime.timezone.utc),\n",
            " 'httpcompression/response_bytes': 92807,\n",
            " 'httpcompression/response_count': 2,\n",
            " 'log_count/DEBUG': 5,\n",
            " 'log_count/INFO': 10,\n",
            " 'response_received_count': 2,\n",
            " 'robotstxt/request_count': 1,\n",
            " 'robotstxt/response_count': 1,\n",
            " 'robotstxt/response_status_count/200': 1,\n",
            " 'scheduler/dequeued': 1,\n",
            " 'scheduler/dequeued/memory': 1,\n",
            " 'scheduler/enqueued': 1,\n",
            " 'scheduler/enqueued/memory': 1,\n",
            " 'start_time': datetime.datetime(2024, 9, 25, 11, 36, 17, 909963, tzinfo=datetime.timezone.utc)}\n",
            "2024-09-25 13:36:18 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ]
        }
      ],
      "source": [
        "!scrapy crawl datascience_spider"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHKtqTS95RSx"
      },
      "source": [
        "# MILESTONE 2\n",
        "\n",
        "a) Obtain the link to Web Analytics course by finding the corresponding href.\n",
        "\n",
        "b) Create a new spider _class_ and access to this URL.\n",
        "\n",
        "**TIP**: For this milestone, you need to create a new crawler and give it a different name.\n",
        "\n",
        "c) Print the text inside the _Description of contents: programme_ section.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7xkCvEVXDT2",
        "outputId": "8e790563-1504-4b15-f641-77d5724204cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spider 'web_spider' already exists in module:\n",
            "  scrapy_lab.spiders.web_spider\n"
          ]
        }
      ],
      "source": [
        "# Create a new crawler/spider\n",
        "!scrapy genspider web_spider https://www.uc3m.es/bachelor-degree/data-science"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-yQti1TXDPz",
        "outputId": "e213e3f2-2614-40d6-b6f8-e69d95a59f9a"
      },
      "outputs": [],
      "source": [
        "# a) Obtain the link to Web Analytics course by finding the corresponding href\n",
        "class WebSpiderSpider(scrapy.Spider):\n",
        "    name = \"web_spider\"\n",
        "    allowed_domains = [\"www.uc3m.es\"]\n",
        "    start_urls = [\"https://www.uc3m.es/bachelor-degree/data-science\"]\n",
        "\n",
        "    def parse(self, response):\n",
        "      web_analytics_link = response.xpath('//*[contains(text(), \"Web Analytics\")]/parent::a/@href').get()\n",
        "\n",
        "      # Print the link if found\n",
        "      if web_analytics_link:\n",
        "        print(f\"Web Analytics Link: {web_analytics_link}\")\n",
        "      else:\n",
        "        print(\"Link not found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtEwewGQXDNb",
        "outputId": "da3c746f-16c7-4a5c-8607-41696c4740a0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-09-25 13:36:30 [scrapy.utils.log] INFO: Scrapy 2.11.2 started (bot: scrapy_lab)\n",
            "2024-09-25 13:36:30 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.11.7, cssselect 1.2.0, parsel 1.9.1, w3lib 2.2.1, Twisted 24.7.0, Python 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)], pyOpenSSL 24.2.1 (OpenSSL 3.3.2 3 Sep 2024), cryptography 43.0.1, Platform Windows-10-10.0.22631-SP0\n",
            "2024-09-25 13:36:30 [scrapy.addons] INFO: Enabled addons:\n",
            "[]\n",
            "2024-09-25 13:36:30 [asyncio] DEBUG: Using selector: SelectSelector\n",
            "2024-09-25 13:36:30 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
            "2024-09-25 13:36:30 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop\n",
            "2024-09-25 13:36:30 [scrapy.extensions.telnet] INFO: Telnet Password: 31ec9a1c70b11f83\n",
            "2024-09-25 13:36:30 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2024-09-25 13:36:30 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'BOT_NAME': 'scrapy_lab',\n",
            " 'FEED_EXPORT_ENCODING': 'utf-8',\n",
            " 'NEWSPIDER_MODULE': 'scrapy_lab.spiders',\n",
            " 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n",
            " 'ROBOTSTXT_OBEY': True,\n",
            " 'SPIDER_MODULES': ['scrapy_lab.spiders'],\n",
            " 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n",
            "2024-09-25 13:36:30 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2024-09-25 13:36:30 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2024-09-25 13:36:30 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2024-09-25 13:36:30 [scrapy.core.engine] INFO: Spider opened\n",
            "2024-09-25 13:36:30 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2024-09-25 13:36:30 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2024-09-25 13:36:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.uc3m.es/robots.txt> (referer: None)\n",
            "2024-09-25 13:36:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.uc3m.es/bachelor-degree/data-science> (referer: None)\n",
            "2024-09-25 13:36:31 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2024-09-25 13:36:31 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 462,\n",
            " 'downloader/request_count': 2,\n",
            " 'downloader/request_method_count/GET': 2,\n",
            " 'downloader/response_bytes': 18391,\n",
            " 'downloader/response_count': 2,\n",
            " 'downloader/response_status_count/200': 2,\n",
            " 'elapsed_time_seconds': 0.672485,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2024, 9, 25, 11, 36, 31, 574422, tzinfo=datetime.timezone.utc),\n",
            " 'httpcompression/response_bytes': 92800,\n",
            " 'httpcompression/response_count': 2,\n",
            " 'log_count/DEBUG': 5,\n",
            " 'log_count/INFO': 10,\n",
            " 'response_received_count': 2,\n",
            " 'robotstxt/request_count': 1,\n",
            " 'robotstxt/response_count': 1,\n",
            " 'robotstxt/response_status_count/200': 1,\n",
            " 'scheduler/dequeued': 1,\n",
            " 'scheduler/dequeued/memory': 1,\n",
            " 'scheduler/enqueued': 1,\n",
            " 'scheduler/enqueued/memory': 1,\n",
            " 'start_time': datetime.datetime(2024, 9, 25, 11, 36, 30, 901937, tzinfo=datetime.timezone.utc)}\n",
            "2024-09-25 13:36:31 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ]
        }
      ],
      "source": [
        "!scrapy crawl web_spider"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# b) Create a new spider class and access to this URL; c) Print the text inside the Description of contents: programme section.\n",
        "class WebSpiderSpider(scrapy.Spider):\n",
        "    name = \"web_spider\"\n",
        "    allowed_domains = [\"www.uc3m.es\"]\n",
        "    start_urls = [\"https://aplicaciones.uc3m.es/cpa/generaFicha?&est=350&plan=392&asig=16507&idioma=2\"]\n",
        "\n",
        "    def parse(self, response):\n",
        "        # Locate the selector of \"Description of contents: programme\"\n",
        "        selector = response.xpath('//div[contains(@class, \"panel-heading\") and contains(text(), \"Description of contents: programme\")]')\n",
        "\n",
        "        # The next sibling contains the text:\n",
        "        next_sibling = selector.xpath('following-sibling::*[1]//text()').getall()\n",
        "        print(next_sibling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-09-25 13:36:35 [scrapy.utils.log] INFO: Scrapy 2.11.2 started (bot: scrapy_lab)\n",
            "2024-09-25 13:36:35 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.11.7, cssselect 1.2.0, parsel 1.9.1, w3lib 2.2.1, Twisted 24.7.0, Python 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)], pyOpenSSL 24.2.1 (OpenSSL 3.3.2 3 Sep 2024), cryptography 43.0.1, Platform Windows-10-10.0.22631-SP0\n",
            "2024-09-25 13:36:35 [scrapy.addons] INFO: Enabled addons:\n",
            "[]\n",
            "2024-09-25 13:36:35 [asyncio] DEBUG: Using selector: SelectSelector\n",
            "2024-09-25 13:36:35 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
            "2024-09-25 13:36:35 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop\n",
            "2024-09-25 13:36:35 [scrapy.extensions.telnet] INFO: Telnet Password: e8ffa772a2dadb09\n",
            "2024-09-25 13:36:35 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2024-09-25 13:36:35 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'BOT_NAME': 'scrapy_lab',\n",
            " 'FEED_EXPORT_ENCODING': 'utf-8',\n",
            " 'NEWSPIDER_MODULE': 'scrapy_lab.spiders',\n",
            " 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n",
            " 'ROBOTSTXT_OBEY': True,\n",
            " 'SPIDER_MODULES': ['scrapy_lab.spiders'],\n",
            " 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n",
            "2024-09-25 13:36:36 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2024-09-25 13:36:36 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2024-09-25 13:36:36 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2024-09-25 13:36:36 [scrapy.core.engine] INFO: Spider opened\n",
            "2024-09-25 13:36:36 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2024-09-25 13:36:36 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2024-09-25 13:36:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.uc3m.es/robots.txt> (referer: None)\n",
            "2024-09-25 13:36:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.uc3m.es/bachelor-degree/data-science> (referer: None)\n",
            "2024-09-25 13:36:36 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2024-09-25 13:36:36 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 462,\n",
            " 'downloader/request_count': 2,\n",
            " 'downloader/request_method_count/GET': 2,\n",
            " 'downloader/response_bytes': 18333,\n",
            " 'downloader/response_count': 2,\n",
            " 'downloader/response_status_count/200': 2,\n",
            " 'elapsed_time_seconds': 0.654166,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2024, 9, 25, 11, 36, 36, 850859, tzinfo=datetime.timezone.utc),\n",
            " 'httpcompression/response_bytes': 92800,\n",
            " 'httpcompression/response_count': 2,\n",
            " 'log_count/DEBUG': 5,\n",
            " 'log_count/INFO': 10,\n",
            " 'response_received_count': 2,\n",
            " 'robotstxt/request_count': 1,\n",
            " 'robotstxt/response_count': 1,\n",
            " 'robotstxt/response_status_count/200': 1,\n",
            " 'scheduler/dequeued': 1,\n",
            " 'scheduler/dequeued/memory': 1,\n",
            " 'scheduler/enqueued': 1,\n",
            " 'scheduler/enqueued/memory': 1,\n",
            " 'start_time': datetime.datetime(2024, 9, 25, 11, 36, 36, 196693, tzinfo=datetime.timezone.utc)}\n",
            "2024-09-25 13:36:36 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ]
        }
      ],
      "source": [
        "!scrapy crawl web_spider"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QN4L9SxRP3Hj"
      },
      "source": [
        "# MILESTONE 3\n",
        "\n",
        "a) Modify your code from previous milestones for running both crawlers in the same command line process.\n",
        "\n",
        "\n",
        "b) Instead of printing the results (from Milestone 1 and 2), save them in a file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-09-25 13:47:40 [scrapy.utils.log] INFO: Scrapy 2.11.2 started (bot: scrapybot)\n",
            "2024-09-25 13:47:40 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.11.7, cssselect 1.2.0, parsel 1.9.1, w3lib 2.2.1, Twisted 24.7.0, Python 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)], pyOpenSSL 24.2.1 (OpenSSL 3.3.2 3 Sep 2024), cryptography 43.0.1, Platform Windows-10-10.0.22631-SP0\n",
            "2024-09-25 13:47:40 [scrapy.addons] INFO: Enabled addons:\n",
            "[]\n",
            "2024-09-25 13:47:40 [py.warnings] WARNING: c:\\Users\\algar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scrapy\\utils\\request.py:254: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
            "\n",
            "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
            "\n",
            "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
            "  return cls(crawler)\n",
            "\n",
            "2024-09-25 13:47:40 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
            "2024-09-25 13:47:40 [scrapy.extensions.telnet] INFO: Telnet Password: 5d6affa73db1016b\n",
            "2024-09-25 13:47:40 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2024-09-25 13:47:40 [scrapy.crawler] INFO: Overridden settings:\n",
            "{}\n",
            "2024-09-25 13:47:40 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2024-09-25 13:47:40 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2024-09-25 13:47:40 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2024-09-25 13:47:40 [scrapy.core.engine] INFO: Spider opened\n",
            "2024-09-25 13:47:40 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2024-09-25 13:47:40 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2024-09-25 13:47:40 [scrapy.addons] INFO: Enabled addons:\n",
            "[]\n",
            "2024-09-25 13:47:40 [py.warnings] WARNING: c:\\Users\\algar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scrapy\\utils\\request.py:254: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
            "\n",
            "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
            "\n",
            "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
            "  return cls(crawler)\n",
            "\n",
            "2024-09-25 13:47:40 [scrapy.extensions.telnet] INFO: Telnet Password: d8d3d9cfa19bb883\n",
            "2024-09-25 13:47:41 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2024-09-25 13:47:41 [scrapy.crawler] INFO: Overridden settings:\n",
            "{}\n",
            "2024-09-25 13:47:41 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2024-09-25 13:47:41 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2024-09-25 13:47:41 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2024-09-25 13:47:41 [scrapy.core.engine] INFO: Spider opened\n",
            "2024-09-25 13:47:41 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2024-09-25 13:47:41 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024\n",
            "2024-09-25 13:47:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.uc3m.es/bachelor-degree/data-science> (referer: None)\n",
            "2024-09-25 13:47:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://aplicaciones.uc3m.es/cpa/generaFicha?&est=350&plan=392&asig=16507&idioma=2> (referer: None)\n",
            "2024-09-25 13:47:41 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2024-09-25 13:47:41 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 240,\n",
            " 'downloader/request_count': 1,\n",
            " 'downloader/request_method_count/GET': 1,\n",
            " 'downloader/response_bytes': 17388,\n",
            " 'downloader/response_count': 1,\n",
            " 'downloader/response_status_count/200': 1,\n",
            " 'elapsed_time_seconds': 0.909546,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2024, 9, 25, 11, 47, 41, 900180, tzinfo=datetime.timezone.utc),\n",
            " 'httpcompression/response_bytes': 87281,\n",
            " 'httpcompression/response_count': 1,\n",
            " 'log_count/DEBUG': 3,\n",
            " 'log_count/INFO': 20,\n",
            " 'log_count/WARNING': 2,\n",
            " 'response_received_count': 1,\n",
            " 'scheduler/dequeued': 1,\n",
            " 'scheduler/dequeued/memory': 1,\n",
            " 'scheduler/enqueued': 1,\n",
            " 'scheduler/enqueued/memory': 1,\n",
            " 'start_time': datetime.datetime(2024, 9, 25, 11, 47, 40, 990634, tzinfo=datetime.timezone.utc)}\n",
            "2024-09-25 13:47:41 [scrapy.core.engine] INFO: Spider closed (finished)\n",
            "2024-09-25 13:47:41 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2024-09-25 13:47:41 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 274,\n",
            " 'downloader/request_count': 1,\n",
            " 'downloader/request_method_count/GET': 1,\n",
            " 'downloader/response_bytes': 7739,\n",
            " 'downloader/response_count': 1,\n",
            " 'downloader/response_status_count/200': 1,\n",
            " 'elapsed_time_seconds': 0.90016,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2024, 9, 25, 11, 47, 41, 906778, tzinfo=datetime.timezone.utc),\n",
            " 'httpcompression/response_bytes': 17866,\n",
            " 'httpcompression/response_count': 1,\n",
            " 'log_count/DEBUG': 2,\n",
            " 'log_count/INFO': 13,\n",
            " 'log_count/WARNING': 1,\n",
            " 'response_received_count': 1,\n",
            " 'scheduler/dequeued': 1,\n",
            " 'scheduler/dequeued/memory': 1,\n",
            " 'scheduler/enqueued': 1,\n",
            " 'scheduler/enqueued/memory': 1,\n",
            " 'start_time': datetime.datetime(2024, 9, 25, 11, 47, 41, 6618, tzinfo=datetime.timezone.utc)}\n",
            "2024-09-25 13:47:41 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ]
        }
      ],
      "source": [
        "import scrapy\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "from scrapy.utils.project import get_project_settings\n",
        "\n",
        "class DatascienceSpider(scrapy.Spider):\n",
        "    name = 'datascience_spider'\n",
        "    allowed_domains = ['uc3m.es']\n",
        "    start_urls = ['https://www.uc3m.es/bachelor-degree/data-science']\n",
        "\n",
        "    def parse(self, response):\n",
        "        # Step 1: Find the \"Year 1 - Semester 1\" table by locating the h3 with \"Year 1 - Semester 1\"\n",
        "        table = response.xpath('//h3[contains(text(), \"Year 1 - Semester 1\")]/following-sibling::table')\n",
        "\n",
        "        # Step 2: Extract rows from the table\n",
        "        rows = table.xpath('.//tr')\n",
        "\n",
        "        # Step 3: Open a file to save the results\n",
        "        with open(\"/content/drive/My Drive/datascience_content.txt\", \"w\") as f:\n",
        "            # Step 4: Loop through each row and extract cell data\n",
        "            for row in rows:\n",
        "                # Extract all cells (td or th)\n",
        "                cells = row.xpath('.//td | .//th').xpath('string()').getall()\n",
        "\n",
        "                # Save the row data to the file\n",
        "                f.write(','.join(cells) + '\\n')\n",
        "\n",
        "class WebSpiderSpider(scrapy.Spider):\n",
        "    name = \"web_spider\"\n",
        "    allowed_domains = [\"www.uc3m.es\"]\n",
        "    start_urls = [\"https://aplicaciones.uc3m.es/cpa/generaFicha?&est=350&plan=392&asig=16507&idioma=2\"]\n",
        "\n",
        "    def parse(self, response):\n",
        "        # Locate the selector of \"Description of contents: programme\"\n",
        "        selector = response.xpath('//div[contains(@class, \"panel-heading\") and contains(text(), \"Description of contents: programme\")]')\n",
        "\n",
        "        # The next sibling contains the text:\n",
        "        next_sibling = selector.xpath('following-sibling::*[1]//text()').getall()\n",
        "      \n",
        "        # Save the content to a file\n",
        "        with open(\"/content/drive/My Drive/webspider_content.txt\", \"w\") as f:\n",
        "            f.write('\\n'.join(next_sibling))\n",
        "\n",
        "settings = get_project_settings()\n",
        "process = CrawlerProcess(settings)\n",
        "process.crawl(DatascienceSpider)\n",
        "process.crawl(WebSpiderSpider)\n",
        "process.start()  # the script will block here until all crawling jobs are finished"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
